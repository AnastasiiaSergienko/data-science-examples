{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing and linear regression with Exasol\n",
    "\n",
    "This tutorial is for users who want to use Exasol as a data source for training a linear regression model.\n",
    "However, the data analyzing and processing part can also be interesting to anyone who's looking for the information how to prepare a dataset for a training. \n",
    "\n",
    "In this tutorial we will discuss the following topics:\n",
    "\n",
    "- Part 1. How to import a dataset from CSV file to Exasol database.\n",
    "- Part 2. How to analyze the data in the table using.\n",
    "- Part 3. How to prepare data for using it in linear regression model.\n",
    "- Part 4. How to create and train a model.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The users are assumed to have a basic understanding of Exasol, SQL and also basic Python programming knowledge.\n",
    "\n",
    "## Part 1. Importing data\n",
    "\n",
    "In our tutorial we use `Flights.csv` file which provides information about the U.S. domestic flights from 1987 to 2019 year.\n",
    "The file contains more than 184 millions rows and 109 columns.\n",
    "\n",
    "As the CSV file is too big, - more than 80 GB - we can't work with it directly importing is as a Pandas DataFrame.\n",
    "That's the reason why we need to use a database as a warehouse and also a workbench for a future data transformation process.\n",
    "\n",
    "So, the first step: **we start an Exasol Database**. \n",
    "For this tutorial we installed an Exasol Cloud Image using [Exasol Cloud Wizard](https://cloudtools.exasol.com/#/).\n",
    "If you want to know more about Exasol Cloud Wizard, you can read an article about how to use this tool in our [blog](https://www.exasol.com/en/blog/building-clusters-in-the-sky-exasol-cloud-wizard/).\n",
    "A local Exasol node was not a good option in our case, because a local machine with this node proceeds data significantly slower than the cloud image.\n",
    "\n",
    "The second step: **import data from CSV file to Exasol's table**. \n",
    "\n",
    "1. We create an SQL CREATE TABLE statement and put it in a separate file named `flights.sql`.\n",
    "Our table will have 109 columns according to the CSV file. Here is a part of the query:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE \"FLIGHTS\" (\"YEAR\" INTEGER, \"QUARTER\" INTEGER, \"MONTH\" INTEGER, \"DAY_OF_MONTH\" INTEGER, \"DAY_OF_WEEK\" DECIMAL(1,0), ... , ...);\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. We connect to the Exasol, create a new schema and create a new table using the statement above.\n",
    "\n",
    " For connection and executing queries we use [pyexasol](https://github.com/badoo/pyexasol) library.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "\n",
    "connection = pyexasol.connect(dsn='host:port', user='username', password='password', compression=True)\n",
    "\n",
    "connection.execute(\"CREATE SCHEMA IF NOT EXISTS {schema_name};\".format(schema_name=\"FLIGHTS\"))\n",
    "connection.open_schema(\"FLIGHTS\")\n",
    "create_table_query = open('flights.sql', 'r')\n",
    "for line in create_table_query:\n",
    "    connection.execute(query=line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 3. After table is ready, we are importing the CSV file's content to the table using pyexasol. \n",
    "    The file is stored in Google Storage because of its size, so we only use a link to the file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "connection.execute(\"IMPORT INTO {table_name} FROM CSV AT '{file_path}' FILE '{file_name}' \"\n",
    "            \"COLUMN SEPARATOR = '{column_separator}' SKIP = 1  \"\n",
    "            \"ERRORS INTO error_table (CURRENT_TIMESTAMP) REJECT LIMIT UNLIMITED ERRORS\".format(\n",
    "                table_name=\"FLIGHTS\", file_path=\"https://storage.googleapis.com/our/path/\", file_name=\"flights.single.csv.gz\", column_separator=\",\"))\n",
    "connection.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you should be able to find all the data in the Exasol's table. \n",
    "Below you can see a python class that is doing the process described above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import simplestopwatch\n",
    "\n",
    "\n",
    "class CsvImporter:\n",
    "    def __init__(self, connection):\n",
    "        self.connection = connection\n",
    "\n",
    "    def import_file(self, sql_create_table_file, schema_name, table_name, file_path, file_name, column_separator):\n",
    "        self.__handle_schema(self.connection, schema_name)\n",
    "        self.__execute_create_table(sql_create_table_file)\n",
    "        self.__run_import_command(self.connection, table_name, file_path, file_name, column_separator)\n",
    "\n",
    "    def __execute_create_table(self, sql_create_table_file):\n",
    "        query = open(sql_create_table_file, 'r')\n",
    "        for line in query:\n",
    "            self.connection.execute(query=line)\n",
    "\n",
    "    def __handle_schema(self, connection, schema_name):\n",
    "        connection.execute(\"CREATE SCHEMA IF NOT EXISTS \" + schema_name + \";\")\n",
    "        connection.open_schema(schema_name)\n",
    "\n",
    "    def __run_import_command(self, connection, table_name, file_path, file_name, column_separator):\n",
    "        timer = simplestopwatch.Timer()\n",
    "        connection.execute(\n",
    "            \"IMPORT INTO {table_name} FROM CSV AT '{file_path}' FILE '{file_name}' \"\n",
    "            \"COLUMN SEPARATOR = '{column_separator}' SKIP = 1  \"\n",
    "            \"ERRORS INTO error_table (CURRENT_TIMESTAMP) REJECT LIMIT UNLIMITED ERRORS\".format(\n",
    "                table_name=table_name, file_path=file_path, file_name=file_name, column_separator=column_separator))\n",
    "        timer.stop()\n",
    "        print(\"Imported in \" + str(timer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}