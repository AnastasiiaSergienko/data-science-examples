{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data processing and linear regression with Exasol\n",
    "\n",
    "This tutorial is for users who want to use Exasol as a data source for training a linear regression model. \n",
    "However, the data analyzing and processing part can also be interesting to anyone who's looking for information on how to prepare a dataset for training. \n",
    "\n",
    "In this tutorial, we will discuss the following topics:\n",
    "\n",
    "- Part 1. How to import a dataset from CSV file to Exasol database.\n",
    "- Part 2. How to analyze the data in the table.\n",
    "- Part 3. How to prepare data for using it in a linear regression model.\n",
    "- Part 4. How to create and train a model.\n",
    "\n",
    "The model in this example will predict a flight's arriving delay.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The users are assumed to have a basic understanding of Exasol, SQL and basic Python programming knowledge.\n",
    "\n",
    "## Part 1. Importing data\n",
    "\n",
    "In our tutorial, we use `Flights.csv` file which provides information about U.S. domestic flights from 1987 to the 2019 year.\n",
    "The file has more than 184 million rows and 109 columns.\n",
    "\n",
    "As the CSV file is too big, - more than 80 GB - we can't work with it directly importing is as a Pandas DataFrame.\n",
    "That's why we need to use a database as a warehouse and a workbench for a future data transformation process.\n",
    "\n",
    "So, the first step: **we start an Exasol Database**. \n",
    "For this tutorial, we installed an Exasol Cloud Image using [Exasol Cloud Wizard](https://cloudtools.exasol.com/#/).\n",
    "If you want to know more about Exasol Cloud Wizard, you can read an article about how to use this tool in our [blog](https://www.exasol.com/en/blog/building-clusters-in-the-sky-exasol-cloud-wizard/).\n",
    "A local Exasol node was not a good option in our case because a local machine with this node proceeds data much slower than the cloud image.\n",
    "\n",
    "The second step: **import data from CSV file to Exasol's table**. \n",
    "\n",
    "1. We create an SQL CREATE TABLE statement and put it in a separate file named `flights.sql`.\n",
    "Our table will have 109 columns according to the CSV file. Here is a part of the query:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE \"FLIGHTS\" (\"YEAR\" INTEGER, \"QUARTER\" INTEGER, \"MONTH\" INTEGER, \"DAY_OF_MONTH\" INTEGER, \"DAY_OF_WEEK\" DECIMAL(1,0), ... , ...);\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. We connect to Exasol, create a new schema and create a new table using the statement above.\n",
    "\n",
    " For connection and executing queries, we use [pyexasol](https://github.com/badoo/pyexasol) library.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyexasol\n",
    "\n",
    "connection = pyexasol.connect(dsn='host:port', user='username', password='password', compression=True)\n",
    "\n",
    "connection.execute(\"CREATE SCHEMA IF NOT EXISTS {schema_name};\".format(schema_name=\"FLIGHTS\"))\n",
    "connection.open_schema(\"FLIGHTS\")\n",
    "create_table_query = open('flights.sql', 'r')\n",
    "for line in create_table_query:\n",
    "    connection.execute(query=line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 3. After the table is ready, we are importing the CSV file's content to the table using pyexasol. \n",
    "    The file is stored in Google Storage because of its size, so we only use a link to the file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "connection.execute(\"IMPORT INTO {table_name} FROM CSV AT '{file_path}' FILE '{file_name}' \"\n",
    "            \"COLUMN SEPARATOR = '{column_separator}' SKIP = 1  \"\n",
    "            \"ERRORS INTO error_table (CURRENT_TIMESTAMP) REJECT LIMIT UNLIMITED ERRORS\".format(\n",
    "                table_name=\"FLIGHTS\", file_path=\"https://storage.googleapis.com/our/path/\", file_name=\"flights.single.csv.gz\", column_separator=\",\"))\n",
    "connection.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can find all the data in Exasol's table. \n",
    "Below you can see a python class that is doing the process described above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import simplestopwatch\n",
    "\n",
    "\n",
    "class CsvImporter:\n",
    "    def __init__(self, connection):\n",
    "        self.connection = connection\n",
    "\n",
    "    def import_file(self, sql_create_table_file, schema_name, table_name, file_path, file_name, column_separator):\n",
    "        self.__handle_schema(self.connection, schema_name)\n",
    "        self.__execute_create_table(sql_create_table_file)\n",
    "        self.__run_import_command(self.connection, table_name, file_path, file_name, column_separator)\n",
    "\n",
    "    def __execute_create_table(self, sql_create_table_file):\n",
    "        query = open(sql_create_table_file, 'r')\n",
    "        for line in query:\n",
    "            self.connection.execute(query=line)\n",
    "\n",
    "    def __handle_schema(self, connection, schema_name):\n",
    "        connection.execute(\"CREATE SCHEMA IF NOT EXISTS \" + schema_name + \";\")\n",
    "        connection.open_schema(schema_name)\n",
    "\n",
    "    def __run_import_command(self, connection, table_name, file_path, file_name, column_separator):\n",
    "        timer = simplestopwatch.Timer()\n",
    "        connection.execute(\n",
    "            \"IMPORT INTO {table_name} FROM CSV AT '{file_path}' FILE '{file_name}' \"\n",
    "            \"COLUMN SEPARATOR = '{column_separator}' SKIP = 1  \"\n",
    "            \"ERRORS INTO error_table (CURRENT_TIMESTAMP) REJECT LIMIT UNLIMITED ERRORS\".format(\n",
    "                table_name=table_name, file_path=file_path, file_name=file_name, column_separator=column_separator))\n",
    "        timer.stop()\n",
    "        print(\"Imported in \" + str(timer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2. Analyzing data\n",
    "\n",
    "Now when we have a dataset in the Exasol table, we can start analyzing data.\n",
    "\n",
    "First, we should take a look at the columns list and decide which of them we would use for the model's training.\n",
    "\n",
    "### How do we decide which columns we need?   \n",
    "\n",
    "We just look at the columns one by one and judge whether it can be helpful for prediction or not.\n",
    "A few examples of columns which we decided to use in our model for delay prediction:\n",
    "- Day of week\n",
    "- Departure airport\n",
    "- Arrival airport, etc.\n",
    "\n",
    "And also a few columns which we decided NOT to use:\n",
    "- Flight number: a unique number that represents an exact flight.\n",
    "- Delay reason: we don't need a reason to predict a delay.\n",
    "- Canceled: this information doesn't affect delay.\n",
    "\n",
    "### How to handle columns with similar information?\n",
    "\n",
    "As we don't need the repeated information, we should select only one column which suits best to the model.\n",
    "For example, we have three columns:\n",
    "- Origin Airport: name of an airport as a string.\n",
    "- Origin Airport ID: an identification number assigned by US DOT to identify a unique airport. \n",
    "- Origin Airport Sequence ID: an identification number assigned by US DOT to identify a unique airport at a given point in time.\n",
    "\n",
    "These columns encode the same information but in different ways. We would select the second one - Origin Airport ID, for the model.\n",
    "The first reason - it's numeric. And the second - this code is more stable than the Origin Airport Sequence ID as it can't be changed later.\n",
    "The first reason - it's numeric. And the second - this code is more stable than the Origin Airport Sequence ID as it can't be changed later.\n",
    "\n",
    "### Analyzing selected columns\n",
    "\n",
    "The next step is to collect data about the content of the selected columns. \n",
    "Here is a small list which can give you an idea of how to analyze a column:\n",
    "\n",
    "1. How many null values does the column contain?\n",
    "2. How are the data represented in the column: string, number, date, etc?\n",
    "3. How many distinct values does the column contain?\n",
    "4. What are the maximum and minimum values (for numbers)?\n",
    "\n",
    "For our example, we collected data using `pyexasol` and then created charts with `plotly` library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as plotly\n",
    "import pyexasol\n",
    "from plotly.graph_objs._figure import Figure\n",
    "from pyexasol import ExaConnection\n",
    "\n",
    "\n",
    "class ColumnStatisticCollector:\n",
    "    def get_column_statistic(self, connection: ExaConnection, schema_name: str, table_name: str,\n",
    "                             column_name: str) -> Figure:\n",
    "        connection.open_schema(schema_name)\n",
    "        sum_of_distinct_values = self.__get_query_result(connection,\n",
    "                                                         'SELECT COUNT (DISTINCT \"{column_name}\") FROM {table_name}'\n",
    "                                                         .format(column_name=column_name, table_name=table_name))\n",
    "        sum_of_nulls = self.__get_query_result(connection,\n",
    "                                               'SELECT COUNT(*) FROM  {table_name} WHERE \"{column_name}\" IS NULL'\n",
    "                                               .format(table_name=table_name, column_name=column_name))\n",
    "        max_value = self.__get_query_result(connection, 'SELECT MAX(\"{column_name}\") FROM {table_name}'\n",
    "                                            .format(column_name=column_name, table_name=table_name))\n",
    "        min_value = self.__get_query_result(connection, 'SELECT MIN(\"{column_name}\") FROM {table_name}'\n",
    "                                            .format(column_name=column_name, table_name=table_name))\n",
    "\n",
    "        result_set = connection.export_to_pandas(\n",
    "            'SELECT DISTINCT \"{column_name}\", COUNT(\"{column_name}\") AS sum_of_distinct_values FROM {table_name} '\n",
    "            'GROUP BY \"{column_name}\" ORDER BY \"{column_name}\";'\n",
    "                .format(column_name=column_name, table_name=table_name))\n",
    "\n",
    "        bar = plotly.bar(result_set, x=(column_name), y=\"SUM_OF_DISTINCT_VALUES\",\n",
    "                         title=\"Column: \" + column_name + \", sum of dist values=\" + str(\n",
    "                             sum_of_distinct_values) + \", nulls=\" + str(sum_of_nulls) + \", max value=\" + str(\n",
    "                             max_value) + \", min value=\" + str(min_value))\n",
    "        bar.layout.xaxis.type = 'category'\n",
    "        return bar\n",
    "\n",
    "    def __get_query_result(self, connection: pyexasol.connection, query: str):\n",
    "        iterable_query_result = connection.execute(query)\n",
    "        counter = 0\n",
    "        for row in iterable_query_result:\n",
    "            counter = row[0]\n",
    "        return counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And here is an example of a chart:\n",
    "<img src=\"img/img1.png\">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3. Data pre-processing\n",
    "\n",
    "After analyzing all the data, we created a new table containing only the columns which we consider can be used for the training and prediction of the linear regression.\n",
    "\n",
    "### Cleaning data\n",
    "\n",
    "In the new table we also made a few changes to clean the data:\n",
    "\n",
    "- Removed a few rows where important values contain NULLs. \n",
    "  \n",
    "  In our table, the data were relatively clean, and we removed less than 10 rows with NULL values of unknown origin.\n",
    "  \n",
    "- Replaces NULL values with new values. \n",
    "   \n",
    "  For columns `origin state fips` and `destination state fips` we replaced NULLs with a new value which we can interpret as a 'place outside the USA' as the NULLs in the mentioned columns belonged to the cities not included into the USA.\n",
    "  \n",
    "  We also replace NULLs with 0 values for the `delay` column as we assumed that NULLs mean arriving without delay.\n",
    "\n",
    "- Removed duplicated row.\n",
    "\n",
    "  We consider the rows that contain the same a) `flight number`, b) `flight date`, c) `origin airport id`, d) `destination airport id`, e) `airline id` are duplicates. So we removed duplicated rows and left only one that has the highest value for `arriving delay`. \n",
    "  \n",
    "  Here is an example of a query which looks for duplicates in the table: "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SELECT * FROM TEST.FLIGHTS a\n",
    "JOIN (SELECT \"ORIGIN_AIRPORT_ID\", COUNT(\"ORIGIN_AIRPORT_ID\"), \"FLIGHT_NUM\", COUNT(\"FLIGHT_NUM\"), \"AIRLINE_ID\", COUNT(\"AIRLINE_ID\"), \"DEST_AIRPORT_ID\", COUNT(\"DEST_AIRPORT_ID\"), \"FLIGHT_DATE\", COUNT(\"FLIGHT_DATE\")\n",
    "FROM TEST.FLIGHTS \n",
    "GROUP BY \"ORIGIN_AIRPORT_ID\", \"FLIGHT_NUM\", \"AIRLINE_ID\", \"DEST_AIRPORT_ID\", \"FLIGHT_DATE\"\n",
    "HAVING (COUNT(\"ORIGIN_AIRPORT_ID\") > 1) AND (COUNT(\"FLIGHT_NUM\") > 1) AND (COUNT(\"AIRLINE_ID\")>1) AND (COUNT (\"DEST_AIRPORT_ID\")>1) AND (COUNT(\"FLIGHT_DATE\")>1) ) b\n",
    "ON a.\"ORIGIN_AIRPORT_ID\" = b.\"ORIGIN_AIRPORT_ID\" AND a.\"FLIGHT_NUM\" = b.\"FLIGHT_NUM\" AND a.\"AIRLINE_ID\" = b.\"AIRLINE_ID\" AND a.\"DEST_AIRPORT_ID\" = b.\"DEST_AIRPORT_ID\" AND a.\"FLIGHT_DATE\" = b.\"FLIGHT_DATE\";"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizing data\n",
    "\n",
    "Before loading the data into the model, we need to normalize it. \n",
    "Data normalization is a process in which the data that has different ranges is cast to a certain interval, such as [0,1] or [-1,1]. \n",
    "Normalization can significantly increase the accuracy of the model. If normalization is not applied, it can lead to incorrect and inaccurate results.\n",
    "There are a few different ways to normalize the data depending on the type of data the column contains.\n",
    "\n",
    "We divided our columns into two groups depending on the types of values:\n",
    "\n",
    "1. Numerical values with natural order — values that contain numbers in which sequence is important. \n",
    "    For example, for days of the month, it is important that the 29th comes before the 30th. In this group, we have the following columns: `year`, `month`, `day of month`, `day of week`, `delay`.\n",
    "\n",
    "2. Categorical values — here we have values that can be grouped into a limited amount of categories.\n",
    "    A few examples of the columns from this group: `origin airport id`, `origin city id`, `destination state fips` etc.\n",
    "\n",
    "We are going to normalize the values of these two groups separately, and we can use different approaches depending on the amount of input data.\n",
    "\n",
    "#### How to normalize a small dataset?\n",
    "\n",
    "For a small dataset which can be stored in memory in full size, we can use a `sklearn.preprocessing` library to normalize the data.\n",
    "\n",
    "For categorical data, we apply `LabelEncoder`. LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and N (number of unique categories - 1).\n",
    "For numerical values with the natural order, we use `MinMaxScaler`. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "\n",
    "We created a small utility class that encodes all the columns and creates a new table with encoded data for us.\n",
    "Use this class when a table has no more than 10.000.000 rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import pandas\n",
    "from pandas import DataFrame\n",
    "from pyexasol import ExaConnection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class SmallDataTransformer:\n",
    "    \"\"\"\n",
    "    Use this class when a table contains less than 10.000.000 rows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, connection: ExaConnection):\n",
    "        self.connection = connection\n",
    "\n",
    "    def transform(self, schema_name: str, origin_table_name: str, final_table_name: str,\n",
    "                  min_max_scaling_columns_list: list,\n",
    "                  categorical_columns_list: list, all_columns_with_types_list: str) -> None:\n",
    "        self.connection.open_schema(schema_name)\n",
    "        pandas_result_set = self.connection.export_to_pandas(\n",
    "            \"SELECT * FROM {table_name}\".format(table_name=origin_table_name))\n",
    "        encoded_categorical_columns_data_frame = self.__encode_as_labels(categorical_columns_list, pandas_result_set)\n",
    "        encoded_numerical_columns_data_frame = pandas.DataFrame.from_records(\n",
    "            self.__encode_as_max_min(min_max_scaling_columns_list,\n",
    "                                     pandas_result_set))\n",
    "        all_columns_data_frame = pandas.concat(\n",
    "            [encoded_categorical_columns_data_frame, encoded_numerical_columns_data_frame],\n",
    "            axis=1)\n",
    "        self.connection.execute(\n",
    "            'CREATE OR REPLACE TABLE {final_table_name} ({all_columns_with_types_list});'\n",
    "                .format(final_table_name=final_table_name, all_columns_with_types_list=all_columns_with_types_list))\n",
    "        self.connection.import_from_pandas(all_columns_data_frame, final_table_name)\n",
    "\n",
    "    def __encode_as_labels(self, categorical_columns_list: list, columns_data_frame: DataFrame) -> Any:\n",
    "        if len(categorical_columns_list) != 0:\n",
    "            categorical_columns_data_frame = columns_data_frame[categorical_columns_list]\n",
    "            if not categorical_columns_data_frame.empty:\n",
    "                return categorical_columns_data_frame.apply(LabelEncoder().fit_transform)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __encode_as_max_min(self, min_max_scaling_columns_list: list, columns_data_frame: DataFrame):\n",
    "        if len(min_max_scaling_columns_list) != 0:\n",
    "            min_max_columns_data_frame = columns_data_frame[min_max_scaling_columns_list]\n",
    "            if not min_max_columns_data_frame.empty:\n",
    "                return MinMaxScaler().fit_transform(min_max_columns_data_frame)\n",
    "        else:\n",
    "            return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How to normalize a big dataset?\n",
    "\n",
    "For a bigger dataset, it is preferable to use SQL directly for pre-processing data.\n",
    "So we are going to apply the same `min-max scaling` and `label encoding` techniques, but to do it without ready-to-use libraries.\n",
    "\n",
    "1. Min-max scaling \n",
    "\n",
    "    To apply the `min-max scaling` to the column we need to find the minimum and maximum values for the whole dataset, but then we can use scaling to batches of any size. \n",
    "    For example, we use the following query to encode the first 100 lines of the column `day of week`.\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SELECT 1.00 * (\"DAY_OF_WEEK\" - MIN(\"DAY_OF_WEEK\") OVER()) / (MAX(\"DAY_OF_WEEK\") OVER () - MIN(\"DAY_OF_WEEK\") OVER ()) FROM TEST.FILTERED_FLIGHTS LIMIT 100;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Label encoding\n",
    "    \n",
    "    With the following two steps we apply the label encoding to a categorical column: \n",
    "\n",
    "    1. We find all distinct values in the column and assign ids to them starting from 0. In our case, we saved this data to a new table.\n",
    "    2. We replace all original values with ids.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE TEST.DEST_AIRPORT_ID_CATEGORIES AS SELECT rownum - 1 AS id, DEST_AIRPORT_ID FROM (SELECT DISTINCT DEST_AIRPORT_ID FROM TEST.FILTERED_FLIGHTS);\n",
    " \n",
    "SELECT DEST_AIRPORT_ID_CATEGORIES.id, FILTERED_FLIGHTS.DEST_AIRPORT_ID FROM TEST.FILTERED_FLIGHTS JOIN TEST.DEST_AIRPORT_ID_CATEGORIES ON FILTERED_FLIGHTS.DEST_AIRPORT_ID = DEST_AIRPORT_ID_CATEGORIES.DEST_AIRPORT_ID;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As encoding of columns one by one takes a lot of time, we created a python script that generates a query, encodes all the columns and writes them into a new table.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyexasol import ExaConnection\n",
    "\n",
    "\n",
    "class LargeDataTransformer:\n",
    "    def __init__(self, connection: ExaConnection):\n",
    "        self.connection = connection\n",
    "\n",
    "    def transform(self, schema_name: str, origin_table_name: str, new_table_name: str,\n",
    "                  min_max_scaling_column_list: list,\n",
    "                  categorical_columns_list: list):\n",
    "        self.connection.open_schema(schema_name)\n",
    "        select_subquery = self.__create_select_subquery(origin_table_name,\n",
    "                                                        categorical_columns_list, min_max_scaling_column_list)\n",
    "        query = \"CREATE OR REPLACE TABLE {new_table_name} AS {select_subquery}\".format(new_table_name=new_table_name,\n",
    "                                                                                       select_subquery=select_subquery)\n",
    "        print(query)\n",
    "        self.connection.execute(query)\n",
    "\n",
    "    def __create_select_subquery(self, origin_table_name: str,\n",
    "                                 categorical_columns_list: list, min_max_scaling_column_list: list) -> str:\n",
    "        select_subquery = \"SELECT {max_min_scaling_columns_query}{comma}{categorical_columns_query} \" \\\n",
    "                          \"FROM {origin_table_name} {categorical_columns_postfix}\"\n",
    "        max_min_scaling_columns_query = self.__generate_query_for_min_max_scaling_columns(min_max_scaling_column_list)\n",
    "        comma = \", \" if len(max_min_scaling_columns_query) > 0 and len(categorical_columns_list) > 0 else \"\"\n",
    "        categorical_columns_query = \"\"\n",
    "        categorical_columns_postfix = \"\"\n",
    "        if len(categorical_columns_list) > 0:\n",
    "            categories_table_name_postfix = '_CATEGORIES'\n",
    "            self.__create_categorical_tables(origin_table_name, categorical_columns_list, categories_table_name_postfix)\n",
    "            categorical_columns_query = self.__generate_query_for_categorical_columns(categorical_columns_list,\n",
    "                                                                                      categories_table_name_postfix)\n",
    "            categorical_columns_postfix = self.___generate_categorical_columns_postfix(origin_table_name,\n",
    "                                                                                       categorical_columns_list,\n",
    "                                                                                       categories_table_name_postfix)\n",
    "        return select_subquery.format(max_min_scaling_columns_query=max_min_scaling_columns_query,\n",
    "                                      comma=comma,\n",
    "                                      categorical_columns_query=categorical_columns_query,\n",
    "                                      origin_table_name=origin_table_name,\n",
    "                                      categorical_columns_postfix=categorical_columns_postfix)\n",
    "\n",
    "    def __generate_query_for_min_max_scaling_columns(self, min_max_scaling_column_list):\n",
    "        individual_columns_queries = []\n",
    "        for column in min_max_scaling_column_list:\n",
    "            individual_columns_queries.append(self.__generate_min_max_scale_column_query(column))\n",
    "        return ', '.join(individual_columns_queries)\n",
    "\n",
    "    def __generate_min_max_scale_column_query(self, column_name: str) -> str:\n",
    "        return '1.00 * (\"{column_name}\" - MIN(\"{column_name}\") OVER()) / (MAX(\"{column_name}\") OVER () - MIN(\"{column_name}\") OVER ()) AS \"{column_name}\"'.format(\n",
    "            column_name=column_name)\n",
    "\n",
    "    def __create_categorical_tables(self, origin_table_name: str,\n",
    "                                    categorical_columns_list: list, categories_table_name_postfix: str):\n",
    "        for column_name in categorical_columns_list:\n",
    "            categories_table_name = column_name + categories_table_name_postfix\n",
    "            self.connection.execute(\n",
    "                'CREATE OR REPLACE TABLE {categories_table_name} AS SELECT rownum - 1 AS id, {column_name} FROM (SELECT DISTINCT {column_name} FROM {origin_table_name})'\n",
    "                    .format(categories_table_name=categories_table_name, column_name=column_name,\n",
    "                            origin_table_name=origin_table_name))\n",
    "\n",
    "    def __generate_query_for_categorical_columns(self, categorical_columns_list: list,\n",
    "                                                 categories_table_name_postfix: str) -> str:\n",
    "        individual_columns_queries = []\n",
    "        for column_name in categorical_columns_list:\n",
    "            table_name = column_name + categories_table_name_postfix\n",
    "            individual_columns_queries.append('{table_name}.id AS {column_name}'.format(table_name=table_name,\n",
    "                                                                                        column_name=column_name))\n",
    "        return ', '.join(individual_columns_queries)\n",
    "\n",
    "    def ___generate_categorical_columns_postfix(self, origin_table_name: str,\n",
    "                                                categorical_columns_list: list,\n",
    "                                                categories_table_name_postfix: str) -> str:\n",
    "        individual_columns_prefixes = []\n",
    "        for column_name in categorical_columns_list:\n",
    "            table_name = column_name + categories_table_name_postfix\n",
    "            individual_columns_prefixes.append(\n",
    "                \"JOIN {table_name} ON {origin_table_name}.{column_name} = {table_name}.{column_name}\".format(\n",
    "                    column_name=column_name, table_name=table_name, origin_table_name=origin_table_name))\n",
    "        return ' '.join(individual_columns_prefixes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}